{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database setup - Single Company table architecture\n",
    "DB_PATH = \"./jobly.db\"\n",
    "\n",
    "def init_database():\n",
    "    \"\"\"Initialize the SQLite database with a single Company table\"\"\"\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Enable WAL mode for better concurrency and performance\n",
    "    cursor.execute(\"PRAGMA journal_mode=WAL\")\n",
    "    cursor.execute(\"PRAGMA synchronous=NORMAL\")\n",
    "    cursor.execute(\"PRAGMA cache_size=10000\")\n",
    "    cursor.execute(\"PRAGMA temp_store=MEMORY\")\n",
    "    \n",
    "    # Create single companies table with all analysis columns\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS companies (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            name TEXT UNIQUE NOT NULL,\n",
    "            is_good BOOLEAN,\n",
    "            is_good_msg TEXT,\n",
    "            is_good_err BOOLEAN,\n",
    "            is_local BOOLEAN,\n",
    "            is_local_msg TEXT,\n",
    "            is_local_err BOOLEAN,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Database initialized with single Company table and WAL mode!\")\n",
    "\n",
    "# Initialize the database\n",
    "init_database()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database helper functions for single table architecture (with better connection management)\n",
    "def get_or_create_company(company_name):\n",
    "    \"\"\"Get company ID, creating the company if it doesn't exist\"\"\"\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH, timeout=30) as conn:\n",
    "            # Enable WAL mode for better concurrency\n",
    "            conn.execute(\"PRAGMA journal_mode=WAL\")\n",
    "            conn.execute(\"PRAGMA synchronous=NORMAL\")\n",
    "            conn.execute(\"PRAGMA cache_size=10000\")\n",
    "            \n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Try to get existing company\n",
    "            cursor.execute(\"SELECT id FROM companies WHERE name = ?\", (company_name,))\n",
    "            result = cursor.fetchone()\n",
    "            \n",
    "            if result:\n",
    "                company_id = result[0]\n",
    "            else:\n",
    "                # Create new company using INSERT OR IGNORE\n",
    "                cursor.execute(\"INSERT OR IGNORE INTO companies (name) VALUES (?)\", (company_name,))\n",
    "                if cursor.lastrowid:\n",
    "                    company_id = cursor.lastrowid\n",
    "                else:\n",
    "                    # Company already exists, get its ID\n",
    "                    cursor.execute(\"SELECT id FROM companies WHERE name = ?\", (company_name,))\n",
    "                    company_id = cursor.fetchone()[0]\n",
    "            \n",
    "            conn.commit()\n",
    "            return company_id\n",
    "    except sqlite3.OperationalError as e:\n",
    "        print(f\"Database error in get_or_create_company: {e}\")\n",
    "        raise\n",
    "\n",
    "def update_company_analysis(company_name, analysis_type, is_positive, message, is_error):\n",
    "    \"\"\"Update company analysis results in the database\"\"\"\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH, timeout=30) as conn:\n",
    "            # Enable WAL mode for better concurrency\n",
    "            conn.execute(\"PRAGMA journal_mode=WAL\")\n",
    "            conn.execute(\"PRAGMA synchronous=NORMAL\")\n",
    "            conn.execute(\"PRAGMA cache_size=10000\")\n",
    "            \n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            if analysis_type == 'is_good':\n",
    "                cursor.execute('''\n",
    "                    UPDATE companies \n",
    "                    SET is_good = ?, is_good_msg = ?, is_good_err = ?, updated_at = CURRENT_TIMESTAMP\n",
    "                    WHERE name = ?\n",
    "                ''', (is_positive, message, is_error, company_name))\n",
    "            elif analysis_type == 'is_local':\n",
    "                cursor.execute('''\n",
    "                    UPDATE companies \n",
    "                    SET is_local = ?, is_local_msg = ?, is_local_err = ?, updated_at = CURRENT_TIMESTAMP\n",
    "                    WHERE name = ?\n",
    "                ''', (is_positive, message, is_error, company_name))\n",
    "            \n",
    "            conn.commit()\n",
    "    except sqlite3.OperationalError as e:\n",
    "        print(f\"Database error in update_company_analysis: {e}\")\n",
    "        raise\n",
    "\n",
    "def get_companies_needing_analysis(analysis_type):\n",
    "    \"\"\"Get companies that need analysis (NULL values)\"\"\"\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH, timeout=10) as conn:\n",
    "            if analysis_type == 'is_good':\n",
    "                query = \"SELECT name FROM companies WHERE is_good IS NULL\"\n",
    "            elif analysis_type == 'is_local':\n",
    "                query = \"SELECT name FROM companies WHERE is_good = 1 AND is_local IS NULL\"\n",
    "            \n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            return df['name'].tolist()\n",
    "    except sqlite3.OperationalError as e:\n",
    "        print(f\"Database error in get_companies_needing_analysis: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_all_companies():\n",
    "    \"\"\"Get all companies with their analysis results\"\"\"\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH, timeout=10) as conn:\n",
    "            query = '''\n",
    "                SELECT id, name, is_good, is_good_msg, is_good_err, \n",
    "                       is_local, is_local_msg, is_local_err, created_at, updated_at\n",
    "                FROM companies \n",
    "                ORDER BY name\n",
    "            '''\n",
    "            \n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            return df\n",
    "    except sqlite3.OperationalError as e:\n",
    "        print(f\"Database error in get_all_companies: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_good_companies():\n",
    "    \"\"\"Get companies where is_good = True\"\"\"\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH, timeout=10) as conn:\n",
    "            query = \"SELECT name FROM companies WHERE is_good = 1\"\n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            return df['name'].tolist()\n",
    "    except sqlite3.OperationalError as e:\n",
    "        print(f\"Database error in get_good_companies: {e}\")\n",
    "        return []\n",
    "\n",
    "def get_final_candidates():\n",
    "    \"\"\"Get companies that are both good AND local/remote\"\"\"\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH, timeout=10) as conn:\n",
    "            query = '''\n",
    "                SELECT name, is_good_msg, is_local_msg\n",
    "                FROM companies \n",
    "                WHERE is_good = 1 AND is_local = 1\n",
    "                ORDER BY name\n",
    "            '''\n",
    "            \n",
    "            df = pd.read_sql_query(query, conn)\n",
    "            return df\n",
    "    except sqlite3.OperationalError as e:\n",
    "        print(f\"Database error in get_final_candidates: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "print(\"Database helper functions loaded with improved connection management!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workflow functions for the complete process\n",
    "def import_companies_from_csv(csv_path, company_column='Company'):\n",
    "    \"\"\"Import companies from CSV and add to database if they don't exist\"\"\"\n",
    "    print(f\"Reading companies from {csv_path}...\")\n",
    "    \n",
    "    # Read CSV\n",
    "    df = pd.read_csv(csv_path, skiprows=2)  # Assuming same format as Connections.csv\n",
    "    companies = df[company_column].unique()\n",
    "    \n",
    "    print(f\"Found {len(companies)} unique companies in CSV\")\n",
    "    \n",
    "    # Add companies to database\n",
    "    added_count = 0\n",
    "    for company_name in companies:\n",
    "        if pd.notna(company_name):  # Skip NaN values\n",
    "            try:\n",
    "                get_or_create_company(company_name)\n",
    "                added_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error adding company '{company_name}': {e}\")\n",
    "    \n",
    "    print(f\"Added {added_count} companies to database\")\n",
    "    return companies\n",
    "\n",
    "def run_analysis_batch(companies, prompt_template, analysis_type):\n",
    "    \"\"\"Run analysis on a batch of companies and update database\"\"\"\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "    counter = 0\n",
    "    \n",
    "    for company_name in companies:\n",
    "        try:\n",
    "            formatted_prompt = prompt_template.format(company_name=company_name)\n",
    "            message = query_claude(formatted_prompt)\n",
    "            \n",
    "            final_text = message.content[-1].text\n",
    "            is_positive = False\n",
    "            is_error = False\n",
    "            \n",
    "            if final_text.endswith('TRUE'):\n",
    "                is_positive = True\n",
    "            elif final_text.endswith('FALSE'):\n",
    "                is_positive = False\n",
    "            else:\n",
    "                is_error = True\n",
    "            \n",
    "            # Update database\n",
    "            update_company_analysis(company_name, analysis_type, is_positive, final_text, is_error)\n",
    "            \n",
    "            total_input_tokens += message.usage.input_tokens\n",
    "            total_output_tokens += message.usage.output_tokens\n",
    "            counter += 1\n",
    "            \n",
    "            print(f\"{counter:3d}/{len(companies)}: {company_name} - {analysis_type}: {is_positive} (Error: {is_error})\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {company_name}: {e}\")\n",
    "            update_company_analysis(company_name, analysis_type, False, str(e), True)\n",
    "    \n",
    "    print(f\"\\nAnalysis complete! Total tokens - Input: {total_input_tokens}, Output: {total_output_tokens}\")\n",
    "\n",
    "def run_complete_workflow(csv_path, good_prompt, local_prompt):\n",
    "    \"\"\"Run the complete workflow: import CSV, run is_good analysis, then is_local analysis\"\"\"\n",
    "    \n",
    "    # Step 1: Import companies from CSV\n",
    "    companies = import_companies_from_csv(csv_path)\n",
    "    \n",
    "    # Step 2: Run is_good analysis on companies that need it\n",
    "    print(\"\\n=== Running is_good analysis ===\")\n",
    "    companies_needing_good_analysis = get_companies_needing_analysis('is_good')\n",
    "    print(f\"Found {len(companies_needing_good_analysis)} companies needing is_good analysis\")\n",
    "    \n",
    "    if companies_needing_good_analysis:\n",
    "        run_analysis_batch(companies_needing_good_analysis, good_prompt, 'is_good')\n",
    "    else:\n",
    "        print(\"All companies already have is_good analysis\")\n",
    "    \n",
    "    # Step 3: Run is_local analysis on good companies that need it\n",
    "    print(\"\\n=== Running is_local analysis ===\")\n",
    "    companies_needing_local_analysis = get_companies_needing_analysis('is_local')\n",
    "    print(f\"Found {len(companies_needing_local_analysis)} good companies needing is_local analysis\")\n",
    "    \n",
    "    if companies_needing_local_analysis:\n",
    "        run_analysis_batch(companies_needing_local_analysis, local_prompt, 'is_local')\n",
    "    else:\n",
    "        print(\"All good companies already have is_local analysis\")\n",
    "    \n",
    "    # Step 4: Show final results\n",
    "    print(\"\\n=== Final Results ===\")\n",
    "    final_candidates = get_final_candidates()\n",
    "    print(f\"Final candidates (good AND local/remote): {len(final_candidates)}\")\n",
    "    \n",
    "    return final_candidates\n",
    "\n",
    "print(\"Workflow functions loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the analysis prompts\n",
    "good_prompt = \"\"\"\n",
    "I am looking for jobs. I am a data scientist looking for a company or non-profit working on a pro-social mission. \n",
    "Some example cause areas: climate change, healthcare, preserving democracy, wealth inequality, education. \n",
    "But I am interested in any others that are for the benefit of the greater good.\n",
    "Can you please let me know if this company fits that above description: {company_name}. \n",
    "If it does meet this conditions, just reply \"TRUE\". If it does not, just reply \"FALSE\". Do not respond with the reasoning for this decision, \n",
    "simply respond \"TRUE\" or \"FALSE\".\n",
    "\"\"\"\n",
    "\n",
    "local_prompt = \"\"\"\n",
    "Can you please let me know if the company {company_name} is either fully remote or based in Colorado?\n",
    "I live in Colorado and can only work for a company that is based in Colorado or fully remote.\n",
    "If it is remote or in Colorado, just reply \"TRUE\". If it is neither, just reply \"FALSE\". \n",
    "Do not respond with the reasoning for this decision, simply respond \"TRUE\" or \"FALSE\".\n",
    "\"\"\"\n",
    "\n",
    "print(\"Analysis prompts defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage and utility functions\n",
    "def show_database_status():\n",
    "    \"\"\"Show current status of the database\"\"\"\n",
    "    all_companies = get_all_companies()\n",
    "    \n",
    "    total = len(all_companies)\n",
    "    good_count = len(all_companies[all_companies['is_good'] == True])\n",
    "    local_count = len(all_companies[all_companies['is_local'] == True])\n",
    "    final_count = len(all_companies[(all_companies['is_good'] == True) & (all_companies['is_local'] == True)])\n",
    "    \n",
    "    print(\"=== Database Status ===\")\n",
    "    print(f\"Total companies: {total}\")\n",
    "    print(f\"Good companies: {good_count}\")\n",
    "    print(f\"Local/remote companies: {local_count}\")\n",
    "    print(f\"Final candidates (good AND local): {final_count}\")\n",
    "    print(f\"Companies needing is_good analysis: {len(get_companies_needing_analysis('is_good'))}\")\n",
    "    print(f\"Good companies needing is_local analysis: {len(get_companies_needing_analysis('is_local'))}\")\n",
    "\n",
    "def export_results_to_csv():\n",
    "    \"\"\"Export final results to CSV\"\"\"\n",
    "    final_candidates = get_final_candidates()\n",
    "    if len(final_candidates) > 0:\n",
    "        filename = f\"./output/final_candidates_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        final_candidates.to_csv(filename, index=False)\n",
    "        print(f\"Exported {len(final_candidates)} final candidates to {filename}\")\n",
    "    else:\n",
    "        print(\"No final candidates to export\")\n",
    "\n",
    "# Example: Run the complete workflow\n",
    "# Uncomment the line below to run the complete process\n",
    "# final_candidates = run_complete_workflow(\"./input/Connections.csv\", good_prompt, local_prompt)\n",
    "\n",
    "print(\"Ready to run! Uncomment the line above to start the complete workflow.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Individual step examples (uncomment to run specific steps)\n",
    "\n",
    "# # Step 1: Import companies from CSV\n",
    "# companies = import_companies_from_csv(\"./input/Connections.csv\")\n",
    "\n",
    "# # Step 2: Run only is_good analysis\n",
    "# companies_needing_good = get_companies_needing_analysis('is_good')\n",
    "# if companies_needing_good:\n",
    "#     run_analysis_batch(companies_needing_good, good_prompt, 'is_good')\n",
    "\n",
    "# # Step 3: Run only is_local analysis (on good companies)\n",
    "# companies_needing_local = get_companies_needing_analysis('is_local')\n",
    "# if companies_needing_local:\n",
    "#     run_analysis_batch(companies_needing_local, local_prompt, 'is_local')\n",
    "\n",
    "# # Check database status\n",
    "# show_database_status()\n",
    "\n",
    "# # View all companies\n",
    "# all_companies = get_all_companies()\n",
    "# print(all_companies.head(10))\n",
    "\n",
    "# # View final candidates\n",
    "# final_candidates = get_final_candidates()\n",
    "# print(final_candidates)\n",
    "\n",
    "# # Export results\n",
    "# export_results_to_csv()\n",
    "\n",
    "# print(\"Individual step examples ready! Uncomment the lines above to run specific steps.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix database lock issues and add better connection management\n",
    "import os\n",
    "import time\n",
    "\n",
    "def force_unlock_database():\n",
    "    \"\"\"Force unlock the database by closing any open connections\"\"\"\n",
    "    try:\n",
    "        # Try to connect and immediately close to release any locks\n",
    "        conn = sqlite3.connect(DB_PATH, timeout=1)\n",
    "        conn.close()\n",
    "        print(\"Database unlocked successfully!\")\n",
    "    except sqlite3.OperationalError as e:\n",
    "        print(f\"Database still locked: {e}\")\n",
    "        print(\"Try restarting your kernel (Kernel -> Restart) and run the cells again\")\n",
    "\n",
    "def check_database_status():\n",
    "    \"\"\"Check if database is accessible\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_PATH, timeout=5)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM companies\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        conn.close()\n",
    "        print(f\"Database is accessible. Contains {count} companies.\")\n",
    "        return True\n",
    "    except sqlite3.OperationalError as e:\n",
    "        print(f\"Database error: {e}\")\n",
    "        return False\n",
    "\n",
    "# Try to unlock the database\n",
    "force_unlock_database()\n",
    "check_database_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Troubleshooting database lock issues\n",
    "\n",
    "def troubleshoot_database():\n",
    "    \"\"\"Comprehensive database troubleshooting\"\"\"\n",
    "    print(\"=== Database Troubleshooting ===\")\n",
    "    \n",
    "    # Check if database file exists\n",
    "    if os.path.exists(DB_PATH):\n",
    "        print(f\"✓ Database file exists: {DB_PATH}\")\n",
    "        file_size = os.path.getsize(DB_PATH)\n",
    "        print(f\"  File size: {file_size} bytes\")\n",
    "    else:\n",
    "        print(f\"✗ Database file not found: {DB_PATH}\")\n",
    "        return\n",
    "    \n",
    "    # Try to access database\n",
    "    if check_database_status():\n",
    "        print(\"✓ Database is accessible\")\n",
    "    else:\n",
    "        print(\"✗ Database is locked or corrupted\")\n",
    "        print(\"\\nTry these solutions:\")\n",
    "        print(\"1. Restart your Jupyter kernel (Kernel -> Restart)\")\n",
    "        print(\"2. Close any other programs that might be using the database\")\n",
    "        print(\"3. Delete the database file and reinitialize:\")\n",
    "        print(f\"   os.remove('{DB_PATH}')\")\n",
    "        print(\"   init_database()\")\n",
    "\n",
    "def reset_database():\n",
    "    \"\"\"Reset the database (use with caution!)\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(DB_PATH):\n",
    "            os.remove(DB_PATH)\n",
    "            print(f\"Removed database file: {DB_PATH}\")\n",
    "        \n",
    "        init_database()\n",
    "        print(\"Database reset successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error resetting database: {e}\")\n",
    "\n",
    "# Run troubleshooting\n",
    "troubleshoot_database()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggressive database unlock methods\n",
    "import subprocess\n",
    "import signal\n",
    "import psutil\n",
    "\n",
    "def force_kill_database_connections():\n",
    "    \"\"\"Force kill any processes that might be holding the database\"\"\"\n",
    "    try:\n",
    "        # Find processes using the database file\n",
    "        for proc in psutil.process_iter(['pid', 'name', 'open_files']):\n",
    "            try:\n",
    "                if proc.info['open_files']:\n",
    "                    for file_info in proc.info['open_files']:\n",
    "                        if DB_PATH in file_info.path:\n",
    "                            print(f\"Found process {proc.info['name']} (PID: {proc.info['pid']}) using database\")\n",
    "                            # Don't actually kill it, just report\n",
    "            except (psutil.NoSuchProcess, psutil.AccessDenied, psutil.ZombieProcess):\n",
    "                pass\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking processes: {e}\")\n",
    "\n",
    "def aggressive_unlock():\n",
    "    \"\"\"Try multiple methods to unlock the database\"\"\"\n",
    "    print(\"=== Aggressive Database Unlock ===\")\n",
    "    \n",
    "    # Method 1: Try with WAL mode\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH, timeout=1) as conn:\n",
    "            conn.execute(\"PRAGMA journal_mode=WAL\")\n",
    "            conn.execute(\"PRAGMA wal_checkpoint(TRUNCATE)\")\n",
    "            conn.close()\n",
    "        print(\"✓ WAL checkpoint completed\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ WAL checkpoint failed: {e}\")\n",
    "    \n",
    "    # Method 2: Try to force close all connections\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH, timeout=1) as conn:\n",
    "            conn.execute(\"PRAGMA busy_timeout=1000\")\n",
    "            conn.execute(\"SELECT 1\")\n",
    "            conn.close()\n",
    "        print(\"✓ Connection test passed\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Connection test failed: {e}\")\n",
    "    \n",
    "    # Method 3: Check for lock files\n",
    "    lock_files = [DB_PATH + \"-wal\", DB_PATH + \"-shm\", DB_PATH + \"-journal\"]\n",
    "    for lock_file in lock_files:\n",
    "        if os.path.exists(lock_file):\n",
    "            try:\n",
    "                os.remove(lock_file)\n",
    "                print(f\"✓ Removed lock file: {lock_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Could not remove {lock_file}: {e}\")\n",
    "\n",
    "def nuclear_option():\n",
    "    \"\"\"Nuclear option: backup and recreate database\"\"\"\n",
    "    print(\"=== Nuclear Option: Backup and Recreate ===\")\n",
    "    \n",
    "    if not os.path.exists(DB_PATH):\n",
    "        print(\"Database doesn't exist, creating new one...\")\n",
    "        init_database()\n",
    "        return\n",
    "    \n",
    "    # Create backup\n",
    "    backup_path = f\"{DB_PATH}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    try:\n",
    "        # Try to backup using SQLite's backup command\n",
    "        with sqlite3.connect(DB_PATH, timeout=1) as source:\n",
    "            with sqlite3.connect(backup_path) as backup:\n",
    "                source.backup(backup)\n",
    "        print(f\"✓ Database backed up to: {backup_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Backup failed: {e}\")\n",
    "        print(\"Proceeding with recreation anyway...\")\n",
    "    \n",
    "    # Remove old database\n",
    "    try:\n",
    "        os.remove(DB_PATH)\n",
    "        print(\"✓ Old database removed\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Could not remove old database: {e}\")\n",
    "    \n",
    "    # Remove any lock files\n",
    "    lock_files = [DB_PATH + \"-wal\", DB_PATH + \"-shm\", DB_PATH + \"-journal\"]\n",
    "    for lock_file in lock_files:\n",
    "        if os.path.exists(lock_file):\n",
    "            try:\n",
    "                os.remove(lock_file)\n",
    "                print(f\"✓ Removed {lock_file}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Create new database\n",
    "    init_database()\n",
    "    print(\"✓ New database created\")\n",
    "\n",
    "# # Run aggressive unlock\n",
    "# force_kill_database_connections()\n",
    "# aggressive_unlock()\n",
    "\n",
    "# print(\"\\nIf still locked, try the nuclear option:\")\n",
    "# print(\"nuclear_option()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple manual unlock (no extra packages needed)\n",
    "def simple_manual_unlock():\n",
    "    \"\"\"Simple manual unlock without external dependencies\"\"\"\n",
    "    print(\"=== Simple Manual Unlock ===\")\n",
    "    \n",
    "    # List all database-related files\n",
    "    db_dir = os.path.dirname(DB_PATH) or \".\"\n",
    "    db_name = os.path.basename(DB_PATH)\n",
    "    \n",
    "    print(f\"Looking for database files in: {db_dir}\")\n",
    "    print(f\"Database name: {db_name}\")\n",
    "    \n",
    "    # Find all related files\n",
    "    related_files = []\n",
    "    for file in os.listdir(db_dir):\n",
    "        if file.startswith(db_name):\n",
    "            related_files.append(os.path.join(db_dir, file))\n",
    "    \n",
    "    print(f\"Found database files: {related_files}\")\n",
    "    \n",
    "    # Try to remove lock files manually\n",
    "    lock_extensions = ['.wal', '.shm', '.journal']\n",
    "    for ext in lock_extensions:\n",
    "        lock_file = DB_PATH + ext\n",
    "        if os.path.exists(lock_file):\n",
    "            try:\n",
    "                os.remove(lock_file)\n",
    "                print(f\"✓ Removed {lock_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Could not remove {lock_file}: {e}\")\n",
    "    \n",
    "    # Try a simple connection test\n",
    "    try:\n",
    "        conn = sqlite3.connect(DB_PATH, timeout=1)\n",
    "        conn.execute(\"SELECT 1\")\n",
    "        conn.close()\n",
    "        print(\"✓ Database is now accessible!\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Database still locked: {e}\")\n",
    "        return False\n",
    "\n",
    "def quick_fix():\n",
    "    \"\"\"Quick fix: just delete and recreate\"\"\"\n",
    "    print(\"=== Quick Fix: Delete and Recreate ===\")\n",
    "    \n",
    "    # Remove database file\n",
    "    if os.path.exists(DB_PATH):\n",
    "        try:\n",
    "            os.remove(DB_PATH)\n",
    "            print(f\"✓ Removed {DB_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Could not remove database: {e}\")\n",
    "            return False\n",
    "    \n",
    "    # Remove any lock files\n",
    "    for ext in ['.wal', '.shm', '.journal']:\n",
    "        lock_file = DB_PATH + ext\n",
    "        if os.path.exists(lock_file):\n",
    "            try:\n",
    "                os.remove(lock_file)\n",
    "                print(f\"✓ Removed {lock_file}\")\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Create new database\n",
    "    init_database()\n",
    "    print(\"✓ New database created\")\n",
    "    return True\n",
    "\n",
    "# Try the simple approach first\n",
    "simple_manual_unlock()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test database access after unlock\n",
    "print(\"=== Testing Database Access ===\")\n",
    "\n",
    "try:\n",
    "    # Test basic connection\n",
    "    with sqlite3.connect(DB_PATH, timeout=5) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM companies\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        print(f\"✓ Database is accessible! Contains {count} companies.\")\n",
    "        \n",
    "        # Test a simple query\n",
    "        cursor.execute(\"SELECT name FROM companies LIMIT 3\")\n",
    "        companies = cursor.fetchall()\n",
    "        if companies:\n",
    "            print(f\"✓ Sample companies: {[c[0] for c in companies]}\")\n",
    "        else:\n",
    "            print(\"✓ Database is empty (no companies yet)\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"✗ Database still has issues: {e}\")\n",
    "\n",
    "print(\"\\nDatabase should now be working! You can proceed with your analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient batch migration functions (to prevent database locks)\n",
    "def batch_migrate_companies(companies_list):\n",
    "    \"\"\"Efficiently migrate a list of companies in a single transaction\"\"\"\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH, timeout=30) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Use INSERT OR IGNORE to avoid duplicates\n",
    "            cursor.executemany(\n",
    "                \"INSERT OR IGNORE INTO companies (name) VALUES (?)\",\n",
    "                [(company,) for company in companies_list if pd.notna(company)]\n",
    "            )\n",
    "            \n",
    "            conn.commit()\n",
    "            print(f\"✓ Batch migrated {len(companies_list)} companies\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Batch migration failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def batch_update_analysis(df, analysis_type):\n",
    "    \"\"\"Efficiently update analysis results in batches\"\"\"\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH, timeout=30) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            if analysis_type == 'is_good':\n",
    "                # Update is_good analysis\n",
    "                for _, row in df.iterrows():\n",
    "                    # Convert string TRUE/FALSE to boolean\n",
    "                    is_good_bool = row['IsGood'] == 'TRUE' if isinstance(row['IsGood'], str) else bool(row['IsGood'])\n",
    "                    is_err_bool = row['IsErr'] == 'TRUE' if isinstance(row['IsErr'], str) else bool(row['IsErr'])\n",
    "                    \n",
    "                    cursor.execute('''\n",
    "                        UPDATE companies \n",
    "                        SET is_good = ?, is_good_msg = ?, is_good_err = ?, updated_at = CURRENT_TIMESTAMP\n",
    "                        WHERE name = ?\n",
    "                    ''', (is_good_bool, row['Message'], is_err_bool, row.name))  # row.name is the company name (index)\n",
    "                    \n",
    "            elif analysis_type == 'is_local':\n",
    "                # Update is_local analysis\n",
    "                for _, row in df.iterrows():\n",
    "                    # Convert string TRUE/FALSE to boolean\n",
    "                    is_local_bool = row['IsLocal'] == 'TRUE' if isinstance(row['IsLocal'], str) else bool(row['IsLocal'])\n",
    "                    is_err_bool = row['IsErr'] == 'TRUE' if isinstance(row['IsErr'], str) else bool(row['IsErr'])\n",
    "                    \n",
    "                    cursor.execute('''\n",
    "                        UPDATE companies \n",
    "                        SET is_local = ?, is_local_msg = ?, is_local_err = ?, updated_at = CURRENT_TIMESTAMP\n",
    "                        WHERE name = ?\n",
    "                    ''', (is_local_bool, row['Message'], is_err_bool, row.name))  # row.name is the company name (index)\n",
    "            \n",
    "            conn.commit()\n",
    "            print(f\"✓ Batch updated {len(df)} {analysis_type} analysis results\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Batch update failed: {e}\")\n",
    "        raise\n",
    "\n",
    "def migrate_csv_data_efficiently():\n",
    "    \"\"\"Migrate CSV data efficiently without causing database locks\"\"\"\n",
    "    print(\"=== Efficient CSV Migration ===\")\n",
    "    \n",
    "    # Migrate first pass (pro_social analysis)\n",
    "    try:\n",
    "        first_pass_df = pd.read_csv(\"./output/dave_connection_companies_first_pass.csv\", index_col=0)\n",
    "        print(f\"Found {len(first_pass_df)} pro_social analysis results\")\n",
    "        \n",
    "        # Batch insert companies\n",
    "        companies_list = first_pass_df.index.tolist()\n",
    "        batch_migrate_companies(companies_list)\n",
    "        \n",
    "        # Batch update analysis\n",
    "        batch_update_analysis(first_pass_df, 'is_good')\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠ First pass CSV not found, skipping...\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ First pass migration failed: {e}\")\n",
    "    \n",
    "    # Migrate second pass (location analysis)\n",
    "    try:\n",
    "        second_pass_df = pd.read_csv(\"./output/dave_connection_companies_second_pass.csv\", index_col=0)\n",
    "        print(f\"Found {len(second_pass_df)} location analysis results\")\n",
    "        \n",
    "        # Batch insert companies (in case some are missing)\n",
    "        companies_list = second_pass_df.index.tolist()\n",
    "        batch_migrate_companies(companies_list)\n",
    "        \n",
    "        # Batch update analysis\n",
    "        batch_update_analysis(second_pass_df, 'is_local')\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"⚠ Second pass CSV not found, skipping...\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Second pass migration failed: {e}\")\n",
    "    \n",
    "    print(\"✓ Migration complete!\")\n",
    "\n",
    "# Run the efficient migration\n",
    "migrate_csv_data_efficiently()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_companies().head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug and fix migration issues\n",
    "def debug_database_migration():\n",
    "    \"\"\"Debug what's in the database vs CSV\"\"\"\n",
    "    print(\"=== Debugging Database Migration ===\")\n",
    "    \n",
    "    # Check CSV data\n",
    "    try:\n",
    "        first_pass_df = pd.read_csv(\"./output/dave_connection_companies_first_pass.csv\", index_col=0)\n",
    "        print(f\"CSV has {len(first_pass_df)} companies\")\n",
    "        print(f\"CSV is_good values: {first_pass_df['IsGood'].value_counts()}\")\n",
    "        print(f\"CSV sample:\")\n",
    "        print(first_pass_df.head(3))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Check database data\n",
    "    try:\n",
    "        db_df = get_all_companies()\n",
    "        print(f\"\\nDatabase has {len(db_df)} companies\")\n",
    "        print(f\"Database is_good values: {db_df['is_good'].value_counts()}\")\n",
    "        print(f\"Database sample:\")\n",
    "        print(db_df[['name', 'is_good', 'is_good_msg']].head(3))\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading database: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Find mismatches\n",
    "    csv_companies = set(first_pass_df.index)\n",
    "    db_companies = set(db_df['name'])\n",
    "    \n",
    "    print(f\"\\nCompanies in CSV but not in DB: {len(csv_companies - db_companies)}\")\n",
    "    print(f\"Companies in DB but not in CSV: {len(db_companies - csv_companies)}\")\n",
    "    \n",
    "    # Check for companies with NULL is_good in DB but have data in CSV\n",
    "    null_in_db = db_df[db_df['is_good'].isnull()]['name'].tolist()\n",
    "    print(f\"Companies with NULL is_good in DB: {len(null_in_db)}\")\n",
    "    \n",
    "    if null_in_db:\n",
    "        print(\"Sample companies with NULL is_good:\")\n",
    "        for company in null_in_db[:5]:\n",
    "            if company in first_pass_df.index:\n",
    "                csv_value = first_pass_df.loc[company, 'IsGood']\n",
    "                print(f\"  {company}: CSV={csv_value}, DB=NULL\")\n",
    "\n",
    "def fix_migration_data():\n",
    "    \"\"\"Fix the migration data by re-running with correct data types\"\"\"\n",
    "    print(\"=== Fixing Migration Data ===\")\n",
    "    \n",
    "    # Re-run the migration with corrected functions\n",
    "    migrate_csv_data_efficiently()\n",
    "    \n",
    "    # Verify the fix\n",
    "    print(\"\\n=== Verification ===\")\n",
    "    db_df = get_all_companies()\n",
    "    print(f\"Companies with is_good=True: {len(db_df[db_df['is_good'] == True])}\")\n",
    "    print(f\"Companies with is_good=False: {len(db_df[db_df['is_good'] == False])}\")\n",
    "    print(f\"Companies with is_good=NULL: {len(db_df[db_df['is_good'].isnull()])}\")\n",
    "\n",
    "# Run debug first\n",
    "debug_database_migration()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete reset and re-migration\n",
    "def reset_and_remigrate():\n",
    "    \"\"\"Reset database and re-migrate all data correctly\"\"\"\n",
    "    print(\"=== Complete Reset and Re-migration ===\")\n",
    "    \n",
    "    # Clear all data from companies table\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH, timeout=30) as conn:\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"DELETE FROM companies\")\n",
    "            conn.commit()\n",
    "            print(\"✓ Cleared all existing data\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error clearing data: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Re-migrate with corrected functions\n",
    "    print(\"\\nRe-migrating data...\")\n",
    "    migrate_csv_data_efficiently()\n",
    "    \n",
    "    # Verify results\n",
    "    print(\"\\n=== Final Verification ===\")\n",
    "    db_df = get_all_companies()\n",
    "    \n",
    "    print(f\"Total companies in database: {len(db_df)}\")\n",
    "    print(f\"Companies with is_good=True: {len(db_df[db_df['is_good'] == True])}\")\n",
    "    print(f\"Companies with is_good=False: {len(db_df[db_df['is_good'] == False])}\")\n",
    "    print(f\"Companies with is_good=NULL: {len(db_df[db_df['is_good'].isnull()])}\")\n",
    "    \n",
    "    if len(db_df[db_df['is_good'].isnull()]) > 0:\n",
    "        print(\"⚠ Still have NULL values - there may be an issue\")\n",
    "        null_companies = db_df[db_df['is_good'].isnull()]['name'].head(5).tolist()\n",
    "        print(f\"Sample NULL companies: {null_companies}\")\n",
    "    else:\n",
    "        print(\"✓ All companies have is_good values!\")\n",
    "\n",
    "# Run the complete reset and re-migration\n",
    "reset_and_remigrate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop any running migration and clean up\n",
    "def stop_migration_and_cleanup():\n",
    "    \"\"\"Stop any running migration and clean up database locks\"\"\"\n",
    "    print(\"=== Stopping Migration and Cleaning Up ===\")\n",
    "    \n",
    "    # Remove any lock files\n",
    "    lock_files = [DB_PATH + \"-wal\", DB_PATH + \"-shm\", DB_PATH + \"-journal\"]\n",
    "    for lock_file in lock_files:\n",
    "        if os.path.exists(lock_file):\n",
    "            try:\n",
    "                os.remove(lock_file)\n",
    "                print(f\"✓ Removed {lock_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"✗ Could not remove {lock_file}: {e}\")\n",
    "    \n",
    "    # Test database access\n",
    "    try:\n",
    "        with sqlite3.connect(DB_PATH, timeout=5) as conn:\n",
    "            conn.execute(\"SELECT 1\")\n",
    "            print(\"✓ Database is accessible\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Database still locked: {e}\")\n",
    "        print(\"Try restarting your kernel (Kernel -> Restart)\")\n",
    "\n",
    "# Run cleanup\n",
    "stop_migration_and_cleanup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "connections_df = pd.read_csv(\"./input/Connections.csv\", skiprows=2)\n",
    "connections_df = connections_df[connections_df['Company'].notna()]\n",
    "# print(connections_df.head())\n",
    "\n",
    "companies = connections_df.Company.unique()\n",
    "# print(companies)\n",
    "print(f\"Successfully read-in connections. Total unique companies: {len(companies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key_file = open(\"./anthropic_api_key.txt\", \"r\")\n",
    "anthropic_api_key = api_key_file.read()\n",
    "\n",
    "client = anthropic.Anthropic(api_key=anthropic_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_claude(prompt, use_search=False):\n",
    "\n",
    "    tools = []\n",
    "    if use_search:\n",
    "        tools = [\n",
    "            {\n",
    "                \"name\": \"web_search\",\n",
    "                \"type\": \"web_search_20250305\",\n",
    "                \"max_uses\": 1\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    return client.beta.messages.create(\n",
    "        model=\"claude-sonnet-4-20250514\",\n",
    "        max_tokens=1024,\n",
    "        temperature=1,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        tools=tools\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_through_companies(companies, prompt_template, bool_col_name='IsGood'):\n",
    "    \n",
    "    result_df = pd.DataFrame(columns=['Company', bool_col_name, 'Message', 'IsErr'])\n",
    "    total_input_tokens = 0\n",
    "    total_output_tokens = 0\n",
    "    counter = 0\n",
    "\n",
    "    for company_name in companies:\n",
    "\n",
    "        formatted_prompt = prompt_template.format(company_name=company_name)\n",
    "        message = query_claude(formatted_prompt)\n",
    "\n",
    "        final_text = message.content[-1].text\n",
    "        bool_col = False\n",
    "        is_err = False\n",
    "\n",
    "        if final_text.endswith('TRUE'):\n",
    "            bool_col = True\n",
    "        elif final_text.endswith('FALSE'):\n",
    "            bool_col = False\n",
    "        else:\n",
    "            is_err = True\n",
    "\n",
    "        result_df.loc[len(result_df)] = [company_name, bool_col, final_text, is_err]\n",
    "\n",
    "        total_input_tokens += message.usage.input_tokens\n",
    "        total_output_tokens += message.usage.output_tokens\n",
    "        counter += 1\n",
    "\n",
    "        print(f\"{counter:3d}/{len(companies)}: Processed company: {company_name}. Input tokens: {message.usage.input_tokens}; Output tokens: {message.usage.output_tokens}; Error: {is_err}\")\n",
    "\n",
    "    return result_df.set_index('Company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_prompt = \"\"\"\n",
    "        I am looking for jobs. I am a data scientist looking for a company or non-profit working on a pro-social mission. \n",
    "        Some example cause areas: climate change, healthcare, preserving democracy, wealth inequality, education. \n",
    "        But I am interested in any others that are for the benefit of the greater good.\n",
    "        Can you please let me know if this company fits that above description: {company_name}. \n",
    "        If it does meet this conditions, just reply \"TRUE\". If it does not, just reply \"FALSE\". Do not respond with the reasoning for this decision, \n",
    "        simply respond \"TRUE\" or \"FALSE\".\n",
    "    \"\"\"\n",
    "\n",
    "result_df = loop_through_companies(companies, good_prompt, bool_col_name='IsGood')\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.read_csv(\"./output/dave_connection_companies_first_pass.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through rows of result_df\n",
    "for index, row in result_df.iterrows():\n",
    "    get_or_create_company(row['Company'])\n",
    "    update_company_analysis(row['Company'], 'is_good', row['IsGood'], row['Message'], row['IsErr'])\n",
    "get_all_companies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_or_create_company('Scope3')\n",
    "update_company_analysis('Scope3', 'is_good', True, 'TRUE', False)\n",
    "get_all_companies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_all_companies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_df = result_df.loc[result_df['IsGood'] == True]\n",
    "good_companies = good_df.index.tolist()\n",
    "print(f\"Good companies: {len(good_companies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_prompt = \"\"\"\n",
    "        Can you please let me know if the company {company_name} is either fully remote or based in Colorado?\n",
    "        I live in Colorado and can only work for a company that is based in Colorado or fully remote.\n",
    "        If it is remote or in Colorado, just reply \"TRUE\". If it is neither, just reply \"FALSE\". \n",
    "        Do not respond with the reasoning for this decision, simply respond \"TRUE\" or \"FALSE\".\n",
    "    \"\"\"\n",
    "\n",
    "loc_result_df = loop_through_companies(good_companies, local_prompt, bool_col_name='IsLocal')\n",
    "loc_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_result_df = pd.read_csv(\"./output/dave_connection_companies_second_pass.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_result_df.loc[loc_result_df['IsLocal'] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loc_result_df.to_csv(\"./output/dave_connection_companies_second_pass.csv\")\n",
    "# result_df.to_csv(\"./output/dave_connection_companies_first_pass.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
